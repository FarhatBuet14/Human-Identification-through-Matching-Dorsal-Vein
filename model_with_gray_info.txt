epochs = 100
batch_size = 8
verbose = 1


#(batch, rows, cols, channels)
input_layer = Input(shape=(300,240,1), name='Input_Layer')

#------------------------  Left Conv Layers  -----------------------------

conv_11 = Conv2D(32, kernel_size = (3, 3), strides = 1, 
               activation = 'relu', padding = 'same')(input_layer)
conv_12 = Conv2D(32, kernel_size = (3, 3), strides = 1, 
               activation = 'relu', padding = 'same')(conv_11)
pool_11 = MaxPooling2D(pool_size=(2, 2))(conv_12)



conv_13 = Conv2D(64, kernel_size = (3, 3), strides = 1, 
               activation = 'relu', padding = 'same')(pool_11)
conv_14 = Conv2D(64, kernel_size = (3, 3), strides = 1, 
               activation = 'relu', padding = 'same')(conv_13)
pool_12 = MaxPooling2D(pool_size=(2, 2))(conv_14)



conv_15 = Conv2D(128, kernel_size = (2, 2), strides = 1, 
               activation = 'relu', padding = 'same')(pool_12)
conv_16 = Conv2D(128, kernel_size = (2, 2), strides = 1, 
               activation = 'relu', padding = 'same')(conv_15)
pool_13 = MaxPooling2D(pool_size=(2, 2))(conv_16)
flat_1 = Flatten()(pool_13)


#------------------------  Densed Layer  --------------------

#merge = concatenate([flat_1, flat_2])

dense_1 = Dense(128, activation='relu')(flat_1)
output_layer = Dense(num_pred_value,
                     name='Output_Layer')(dense_1)


#------------------------  Model Brief  -----------------------------

model = Model(inputs = input_layer, outputs = output_layer)
print(model.summary())
plot_model(model, to_file='shared_input_layer.png')


#################### Compiling the Model ################################
optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(loss='mae', optimizer=optimizer, 
              metrics=['mse'])

#################### Defining the Checkpoints ###########################

learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.25, 
                                            min_lr=0.00001)

wigth  = ModelCheckpoint(weightFile, monitor = 'val_loss' )
callbacks = [wigth, learning_rate_reduction]


############################ Data Augmentation ############################

training_samples = X_train.shape[0]
validation_samples = X_val.shape[0]


history = model.fit(X_train, y_train,
                    validation_data = [X_val , y_val],
                    epochs = epochs, verbose = verbose,
                    callbacks= callbacks)








_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input_Layer (InputLayer)     (None, 300, 240, 1)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 300, 240, 32)      320       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 300, 240, 32)      9248      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 150, 120, 32)      0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 150, 120, 64)      18496     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 150, 120, 64)      36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 75, 60, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 75, 60, 128)       32896     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 75, 60, 128)       65664     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 37, 30, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 142080)            0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               18186368  
_________________________________________________________________
Output_Layer (Dense)         (None, 6)                 774       
=================================================================
Total params: 18,350,694
Trainable params: 18,350,694
Non-trainable params: 0
_________________________________________________________________
None
Train on 475 samples, validate on 318 samples
Epoch 1/100
475/475 [==============================] - 9s 20ms/step - loss: 94.7617 - mean_squared_error: 10550.7060 - val_loss: 33.2463 - val_mean_squared_error: 1735.8748
Epoch 2/100
475/475 [==============================] - 4s 7ms/step - loss: 18.9953 - mean_squared_error: 613.4866 - val_loss: 15.4790 - val_mean_squared_error: 406.0185
Epoch 3/100
475/475 [==============================] - 4s 7ms/step - loss: 14.8869 - mean_squared_error: 356.2362 - val_loss: 14.3930 - val_mean_squared_error: 337.4789
Epoch 4/100
475/475 [==============================] - 3s 7ms/step - loss: 14.6284 - mean_squared_error: 346.8486 - val_loss: 16.0644 - val_mean_squared_error: 396.4119
Epoch 5/100
475/475 [==============================] - 4s 7ms/step - loss: 14.1945 - mean_squared_error: 328.4708 - val_loss: 15.3858 - val_mean_squared_error: 402.7345
Epoch 6/100
475/475 [==============================] - 4s 7ms/step - loss: 14.7814 - mean_squared_error: 342.7896 - val_loss: 14.5421 - val_mean_squared_error: 345.0406

Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
Epoch 7/100
475/475 [==============================] - 4s 7ms/step - loss: 13.0915 - mean_squared_error: 283.1236 - val_loss: 14.3669 - val_mean_squared_error: 335.8964
Epoch 8/100
475/475 [==============================] - 4s 8ms/step - loss: 13.1119 - mean_squared_error: 281.5820 - val_loss: 14.2907 - val_mean_squared_error: 340.2509
Epoch 9/100
475/475 [==============================] - 4s 7ms/step - loss: 13.3409 - mean_squared_error: 292.6698 - val_loss: 14.3809 - val_mean_squared_error: 347.9137
Epoch 10/100
475/475 [==============================] - 3s 7ms/step - loss: 13.1400 - mean_squared_error: 284.1741 - val_loss: 14.3435 - val_mean_squared_error: 339.1374
Epoch 11/100
475/475 [==============================] - 4s 7ms/step - loss: 13.0925 - mean_squared_error: 283.1193 - val_loss: 14.3352 - val_mean_squared_error: 344.3116

Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-05.
Epoch 12/100
475/475 [==============================] - 4s 7ms/step - loss: 13.0115 - mean_squared_error: 280.7507 - val_loss: 14.3068 - val_mean_squared_error: 340.3017
Epoch 13/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0479 - mean_squared_error: 280.0012 - val_loss: 14.2971 - val_mean_squared_error: 340.1279
Epoch 14/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0276 - mean_squared_error: 279.2284 - val_loss: 14.4958 - val_mean_squared_error: 355.5601
Epoch 15/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9945 - mean_squared_error: 279.3878 - val_loss: 14.4042 - val_mean_squared_error: 348.7600
Epoch 16/100
475/475 [==============================] - 4s 7ms/step - loss: 13.0228 - mean_squared_error: 280.0424 - val_loss: 14.2824 - val_mean_squared_error: 338.6542
Epoch 17/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0275 - mean_squared_error: 279.9033 - val_loss: 14.2766 - val_mean_squared_error: 339.0822
Epoch 18/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9759 - mean_squared_error: 278.1882 - val_loss: 14.2838 - val_mean_squared_error: 340.2716
Epoch 19/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0080 - mean_squared_error: 278.9957 - val_loss: 14.2812 - val_mean_squared_error: 334.1566
Epoch 20/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9895 - mean_squared_error: 278.4057 - val_loss: 14.2474 - val_mean_squared_error: 335.4612
Epoch 21/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9627 - mean_squared_error: 279.1537 - val_loss: 14.2834 - val_mean_squared_error: 334.6493
Epoch 22/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0024 - mean_squared_error: 279.5879 - val_loss: 14.2592 - val_mean_squared_error: 338.9145
Epoch 23/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0112 - mean_squared_error: 278.5676 - val_loss: 14.2638 - val_mean_squared_error: 336.8673
Epoch 24/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9862 - mean_squared_error: 277.6359 - val_loss: 14.2683 - val_mean_squared_error: 339.1969
Epoch 25/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9863 - mean_squared_error: 278.6068 - val_loss: 14.3877 - val_mean_squared_error: 348.1125
Epoch 26/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9824 - mean_squared_error: 278.8502 - val_loss: 14.3325 - val_mean_squared_error: 346.0797
Epoch 27/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0229 - mean_squared_error: 280.4436 - val_loss: 14.2333 - val_mean_squared_error: 336.6393
Epoch 28/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9689 - mean_squared_error: 278.2351 - val_loss: 14.2732 - val_mean_squared_error: 339.7778
Epoch 29/100
475/475 [==============================] - 4s 8ms/step - loss: 13.0123 - mean_squared_error: 279.6646 - val_loss: 14.2229 - val_mean_squared_error: 335.1615
Epoch 30/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9643 - mean_squared_error: 276.6656 - val_loss: 14.3408 - val_mean_squared_error: 347.1886
Epoch 31/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9866 - mean_squared_error: 279.6070 - val_loss: 14.2733 - val_mean_squared_error: 339.9286
Epoch 32/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9839 - mean_squared_error: 279.8687 - val_loss: 14.2565 - val_mean_squared_error: 338.5730
Epoch 33/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9839 - mean_squared_error: 278.1830 - val_loss: 14.2255 - val_mean_squared_error: 333.1498
Epoch 34/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9644 - mean_squared_error: 276.6857 - val_loss: 14.4175 - val_mean_squared_error: 349.9818
Epoch 35/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9782 - mean_squared_error: 278.2769 - val_loss: 14.2123 - val_mean_squared_error: 334.7753
Epoch 36/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9557 - mean_squared_error: 277.1495 - val_loss: 14.2272 - val_mean_squared_error: 334.3511
Epoch 37/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9703 - mean_squared_error: 277.4088 - val_loss: 14.2707 - val_mean_squared_error: 340.8740
Epoch 38/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9709 - mean_squared_error: 278.3955 - val_loss: 14.3694 - val_mean_squared_error: 348.6928
Epoch 39/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9619 - mean_squared_error: 277.4362 - val_loss: 14.2491 - val_mean_squared_error: 340.4228
Epoch 40/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9340 - mean_squared_error: 276.9396 - val_loss: 14.2662 - val_mean_squared_error: 339.7565
Epoch 41/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9742 - mean_squared_error: 277.3736 - val_loss: 14.2056 - val_mean_squared_error: 334.0713
Epoch 42/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9344 - mean_squared_error: 274.7461 - val_loss: 14.3935 - val_mean_squared_error: 351.1228
Epoch 43/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9699 - mean_squared_error: 279.3289 - val_loss: 14.2100 - val_mean_squared_error: 333.1515
Epoch 44/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9328 - mean_squared_error: 275.4998 - val_loss: 14.2524 - val_mean_squared_error: 340.5442
Epoch 45/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9615 - mean_squared_error: 278.6385 - val_loss: 14.2075 - val_mean_squared_error: 337.5982
Epoch 46/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9276 - mean_squared_error: 276.9536 - val_loss: 14.2426 - val_mean_squared_error: 333.1230
Epoch 47/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9279 - mean_squared_error: 276.7465 - val_loss: 14.1891 - val_mean_squared_error: 333.0344
Epoch 48/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9142 - mean_squared_error: 276.5766 - val_loss: 14.1726 - val_mean_squared_error: 331.6231
Epoch 49/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9426 - mean_squared_error: 275.6888 - val_loss: 14.1832 - val_mean_squared_error: 333.3062
Epoch 50/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9585 - mean_squared_error: 276.1239 - val_loss: 14.1892 - val_mean_squared_error: 332.9222
Epoch 51/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8775 - mean_squared_error: 274.7789 - val_loss: 14.2402 - val_mean_squared_error: 339.9904
Epoch 52/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8861 - mean_squared_error: 275.6934 - val_loss: 14.2097 - val_mean_squared_error: 331.5262
Epoch 53/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9366 - mean_squared_error: 276.1109 - val_loss: 14.2018 - val_mean_squared_error: 337.8555
Epoch 54/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9443 - mean_squared_error: 276.8390 - val_loss: 14.2329 - val_mean_squared_error: 339.7466
Epoch 55/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8819 - mean_squared_error: 275.8359 - val_loss: 14.3820 - val_mean_squared_error: 350.5273
Epoch 56/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9426 - mean_squared_error: 278.6122 - val_loss: 14.1523 - val_mean_squared_error: 332.1318
Epoch 57/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8900 - mean_squared_error: 274.7028 - val_loss: 14.2174 - val_mean_squared_error: 332.6785
Epoch 58/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9451 - mean_squared_error: 277.7410 - val_loss: 14.2606 - val_mean_squared_error: 342.2911
Epoch 59/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8761 - mean_squared_error: 274.9733 - val_loss: 14.1685 - val_mean_squared_error: 331.9835
Epoch 60/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9395 - mean_squared_error: 275.8745 - val_loss: 14.2300 - val_mean_squared_error: 340.6237
Epoch 61/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8980 - mean_squared_error: 275.0140 - val_loss: 14.1834 - val_mean_squared_error: 336.7864
Epoch 62/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8857 - mean_squared_error: 276.3481 - val_loss: 14.1636 - val_mean_squared_error: 335.0210
Epoch 63/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9184 - mean_squared_error: 275.8433 - val_loss: 14.1760 - val_mean_squared_error: 335.8717
Epoch 64/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8923 - mean_squared_error: 275.3095 - val_loss: 14.1745 - val_mean_squared_error: 335.5251
Epoch 65/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9036 - mean_squared_error: 275.4572 - val_loss: 14.2454 - val_mean_squared_error: 341.7200
Epoch 66/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8982 - mean_squared_error: 275.0490 - val_loss: 14.1355 - val_mean_squared_error: 332.7884
Epoch 67/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8683 - mean_squared_error: 274.2223 - val_loss: 14.1597 - val_mean_squared_error: 332.3565
Epoch 68/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8996 - mean_squared_error: 276.1445 - val_loss: 14.1453 - val_mean_squared_error: 332.0550
Epoch 69/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8865 - mean_squared_error: 273.5836 - val_loss: 14.1342 - val_mean_squared_error: 331.4654
Epoch 70/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8955 - mean_squared_error: 274.5817 - val_loss: 14.3553 - val_mean_squared_error: 349.0174
Epoch 71/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8910 - mean_squared_error: 274.3125 - val_loss: 14.4988 - val_mean_squared_error: 357.3026
Epoch 72/100
475/475 [==============================] - 4s 8ms/step - loss: 12.9069 - mean_squared_error: 276.9822 - val_loss: 14.1464 - val_mean_squared_error: 329.4124
Epoch 73/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8978 - mean_squared_error: 274.2931 - val_loss: 14.2589 - val_mean_squared_error: 341.8263
Epoch 74/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8541 - mean_squared_error: 273.1607 - val_loss: 14.1335 - val_mean_squared_error: 332.9168
Epoch 75/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8625 - mean_squared_error: 273.4407 - val_loss: 14.1585 - val_mean_squared_error: 335.3760
Epoch 76/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8708 - mean_squared_error: 274.8823 - val_loss: 14.1066 - val_mean_squared_error: 329.8003
Epoch 77/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8601 - mean_squared_error: 272.5690 - val_loss: 14.1726 - val_mean_squared_error: 337.3930
Epoch 78/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8464 - mean_squared_error: 272.9760 - val_loss: 14.3479 - val_mean_squared_error: 350.2027
Epoch 79/100
475/475 [==============================] - 4s 7ms/step - loss: 12.9068 - mean_squared_error: 276.4491 - val_loss: 14.1410 - val_mean_squared_error: 335.5806
Epoch 80/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8451 - mean_squared_error: 272.2166 - val_loss: 14.2943 - val_mean_squared_error: 347.3256
Epoch 81/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8172 - mean_squared_error: 273.3178 - val_loss: 14.1113 - val_mean_squared_error: 330.1284
Epoch 82/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8466 - mean_squared_error: 273.2207 - val_loss: 14.2250 - val_mean_squared_error: 343.6822
Epoch 83/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8725 - mean_squared_error: 276.4163 - val_loss: 14.1044 - val_mean_squared_error: 327.6725
Epoch 84/100
475/475 [==============================] - 4s 8ms/step - loss: 12.8746 - mean_squared_error: 273.2087 - val_loss: 14.1526 - val_mean_squared_error: 337.1021
Epoch 85/100
475/475 [==============================] - 3s 7ms/step - loss: 12.8261 - mean_squared_error: 273.3052 - val_loss: 14.1216 - val_mean_squared_error: 329.4933
Epoch 86/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8398 - mean_squared_error: 271.7227 - val_loss: 14.2052 - val_mean_squared_error: 341.0810
Epoch 87/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8114 - mean_squared_error: 273.2884 - val_loss: 14.1125 - val_mean_squared_error: 333.5930
Epoch 88/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8588 - mean_squared_error: 274.2124 - val_loss: 14.0762 - val_mean_squared_error: 326.7787
Epoch 89/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8176 - mean_squared_error: 272.0675 - val_loss: 14.0602 - val_mean_squared_error: 327.8893
Epoch 90/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8400 - mean_squared_error: 273.7713 - val_loss: 14.0746 - val_mean_squared_error: 329.5824
Epoch 91/100
475/475 [==============================] - 3s 7ms/step - loss: 12.7897 - mean_squared_error: 271.2364 - val_loss: 14.1944 - val_mean_squared_error: 340.2297
Epoch 92/100
475/475 [==============================] - 4s 7ms/step - loss: 12.7846 - mean_squared_error: 273.1144 - val_loss: 14.2375 - val_mean_squared_error: 343.1136
Epoch 93/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8196 - mean_squared_error: 271.7619 - val_loss: 14.3589 - val_mean_squared_error: 351.2196
Epoch 94/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8104 - mean_squared_error: 273.2027 - val_loss: 14.2873 - val_mean_squared_error: 345.9350
Epoch 95/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8256 - mean_squared_error: 272.6033 - val_loss: 14.1007 - val_mean_squared_error: 334.3510
Epoch 96/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8424 - mean_squared_error: 273.2719 - val_loss: 14.0913 - val_mean_squared_error: 333.2568
Epoch 97/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8022 - mean_squared_error: 271.5865 - val_loss: 14.1589 - val_mean_squared_error: 338.3700
Epoch 98/100
475/475 [==============================] - 4s 7ms/step - loss: 12.7786 - mean_squared_error: 270.1796 - val_loss: 14.0544 - val_mean_squared_error: 328.0303
Epoch 99/100
475/475 [==============================] - 4s 7ms/step - loss: 12.8329 - mean_squared_error: 273.4435 - val_loss: 14.0944 - val_mean_squared_error: 332.4887
Epoch 100/100
475/475 [==============================] - 4s 7ms/step - loss: 12.7995 - mean_squared_error: 271.4964 - val_loss: 14.2333 - val_mean_squared_error: 344.7285
